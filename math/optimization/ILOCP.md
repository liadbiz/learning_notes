在一个不能solve的'good model'与一定能solve的的'bad model'中，我们一般选择后者

现实世界中，我们应用最多的是线性模型，
其次，是一类特殊的non-linear model. 也就是凸优化。

优化问题的一般形式：

$$
\min f_0(x)
$$

非线性优化几乎涵盖了所有研究中涉及到的操作，因此是非常重要的优化问题。
一般来说，大多数优化问题都是无解的。（现实的问题太过于复杂）

如何证明？其实很简单。在这之前要先定义一些概念：

对于某个具体的问题而言，我们很难说一个方法是解决这个问题的最好方法，但是对于一类具有某种相似特征的问题来说，我们可以这么说。定义某种算法对于某类问题的性能'performance'是在这类问题中的最坏情况上的求解的计算耗费。怎么才算求解呢？在某些情况下，就是求出精确解，但是大多数情况我们都只能求出具有一定精确度$\epsilon$的近似解。

还有一个重要的概念是'oracle'，我们通过他的用处来定义它（因为我也不知道用别的方式怎么定义，有点微妙）：通过oracle可以得到算法需要的信息。

现在我们可以定义计算耗费是什么一个东西了。我们可以从两个方面衡量计算复杂度。

+ 一个是分析复杂度。调用oracle的次数。
+ 另一个是算术复杂度。进行算术计算的次数总和（包括调用oracle和算法执行的部分）

一般第二类复杂度可以通过第一类复杂度得到，所以这本书中主要讲如何得到第一类复杂度。

对于oracle有一个假设，叫'black box'，意思是

+ 对于算法需要的信息都可以通过oracle得到
+ oracle是local的，就是在测试点x附近，oracle给出的答案都是一样的

具体来讲，我们最常用的oracle就是以下几类：

+ zero order oracle: 有f(x)的信息
+ first order oracle: 有f(x), $f^{\prime}(x)$的信息
+ second order oracle: 有f(x), $f^{\prime}(x)$, 和Hessian matrix $f^{\prime \prime}(x)$的信息

在优化中，最基本的几个事实是：
+ 反梯度方向是函数局部下降最快的方向。
+ 局部最小值的一阶最优条件是一阶导数为0，二阶最优条件是一阶导为0，二阶导数大于等于0，也就是

梯度下降方法：
不同梯度下降方法的区别是选择步长的方法不一样。


